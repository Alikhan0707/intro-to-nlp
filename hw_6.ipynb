{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "colab_text_classification_part1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlqOAQmQGXOL",
        "outputId": "c5dcdce2-721c-4815-f228-7f4662f0655c"
      },
      "source": [
        "!wget -O imdb.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1vrQ5czMHoO3pEnmofFskymXMkq_u1dPc\"\n",
        "!unzip imdb.zip\n",
        "!pip -q install eli5\n",
        "!pip -q install spacy\n",
        "!python -m spacy download en"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  imdb.zip\n",
            "  inflating: test.tsv                \n",
            "  inflating: train.tsv               \n",
            "\u001b[K     |████████████████████████████████| 106 kB 7.9 MB/s \n",
            "\u001b[?25hCollecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.2.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF80-qhXt17P"
      },
      "source": [
        "Тут придется авторизировать запрос к гугл драйву, извините."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRdS0CM2rSzV"
      },
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "downloaded = drive.CreateFile({'id': '1d1-5FwxK53ePwygNWeG7jhsOWZbi5HOv'})\n",
        "downloaded.GetContentFile('train_docs.pkl')\n",
        "\n",
        "downloaded = drive.CreateFile({'id': '1MMOY477t965G0C5DtXeREVp0X85UaNq5'})\n",
        "downloaded.GetContentFile('test_docs.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqbkG4qoS0AB"
      },
      "source": [
        "# Классификация текстов\n",
        "\n",
        "Начнём с самого простого - анализа тональности текста.\n",
        "\n",
        "Будем классифицировать отзывы с IMDB на положительные/отрицательные.\n",
        "\n",
        "Датасет взят с http://ai.stanford.edu/~amaas/data/sentiment/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R9e3pctHIV2"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk import PorterStemmer\n",
        "\n",
        "train_df = pd.read_csv(\"train.tsv\", delimiter=\"\\t\")\n",
        "test_df = pd.read_csv(\"test.tsv\", delimiter=\"\\t\")\n",
        "\n",
        "print('Train size = {}'.format(len(train_df)))\n",
        "print('Test size = {}'.format(len(test_df)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oHbJCPPwhjp"
      },
      "source": [
        "Посмотрите глазами на тексты? Какие есть зацепки, как определить, что это за сентимент?\n",
        "\n",
        "Самое простое, как всегда - найти ключевые слова."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DywUCyMLr_TD",
        "outputId": "07a842e2-b071-49e6-9a3c-4ce2e21270ae"
      },
      "source": [
        "#@title Начинаем классифицировать! { vertical-output: true, display-mode: \"form\" }\n",
        "positive_words = 'love', 'great', 'best', 'wonderful' #@param {type:\"raw\"}\n",
        "negative_words = 'worst', 'awful', '1/10', 'crap' #@param {type:\"raw\"}\n",
        "\n",
        "positives_count = test_df.review.apply(lambda text: sum(word in text for word in positive_words))\n",
        "negatives_count = test_df.review.apply(lambda text: sum(word in text for word in negative_words))\n",
        "is_positive = positives_count > negatives_count\n",
        "correct_count = (is_positive == test_df.is_positive).values.sum()\n",
        "\n",
        "accuracy = correct_count / len(test_df)\n",
        "\n",
        "print('Test accuracy = {:.2%}'.format(accuracy))\n",
        "if accuracy > 0.71:\n",
        "    from IPython.display import Image, display\n",
        "    display(Image('https://s3.amazonaws.com/achgen360/t/rmmoZsub.png', width=500))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy = 66.73%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo8HRABxv1kW"
      },
      "source": [
        "**Задание** Придумайте хорошие ключевые слова или фразы и наберите хотя бы 71% точности на тесте (и не забудьте посмотреть на код классификации!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaIrBClMUHZB"
      },
      "source": [
        "**Задание** Кому-нибудь нравятся эти `<br /><br />`? Лично мне - нет. Напишите регулярку, которая будет их удалять"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09OkUmtde6ny",
        "outputId": "148fbf55-ce74-4b08-cdcc-9c981e4510ee"
      },
      "source": [
        "import re\n",
        "\n",
        "pattern = re.compile('<br />')\n",
        "\n",
        "print(train_df['review'].iloc[3])\n",
        "print(pattern.subn(' ', train_df['review'].iloc[3])[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spoilers ahead if you want to call them that...<br /><br />I would almost recommend this film just so people can truly see a 1/10. Where to begin, we'll start from the top...<br /><br />THE STORY: Don't believe the premise - the movie has nothing to do with abandoned cars, and people finially understanding what the mysterious happenings are. It's a draub, basic, go to cabin movie with no intensity or \"effort\".<br /><br />THE SCREENPLAY: I usually give credit to indie screenwriters, it's hard work when you are starting out...but this is crap. The story is flat - it leaves you emotionless the entire movie. The dialogue is extremely weak and predictable boasting lines of \"Woah, you totally freaked me out\" and \"I was wondering if you'd uh...if you'd like to..uh, would you come to the cabin with me?\". It makes me want to rip out all my hair, one strand at a time and feed it to myself.<br /><br />THE CHARACTERS: HOLY CRAP!!!! Some have described the characters as flat, I want to take it one step further and say that they actually have a reverse character arch.. They actually start working on a parallel universe and almost start acting backwards...<br /><br />THE ACTORS: Worse than the characters are the actors. They take already poor written characters and add in terrible high school drama acting. The \"Woah you totally freaked me out\" was said so monotone and slow - like it was dumbed down. I could complain for hours on the actors alone.<br /><br />TECHNICAL: LIGHTING: An eight year old would be disappointed with lighting on this movie. Too shadowy in areas, too bleached in others. The director shouldn't use light as an emotion until he learns how to light a basic scene properly. Baby Steps! SOUND: How many sound guys does it take to make a really shotty sounding movie? 9. With that many sound guys this should sound amazing but quite the opposite has occured. There is one scene in particular that really sticks out, these guys are driving in a car and the sound of the car changes with every camera angle....WEAK! CAMERA: Learn to use it.<br /><br />Anyway, I'm running out of complaining space.....rent it - I dare you...Rent it and learn from it...give it a 1 rating..it deserves it.<br /><br />Signing off... Amanda Christmas\n",
            "Spoilers ahead if you want to call them that...  I would almost recommend this film just so people can truly see a 1/10. Where to begin, we'll start from the top...  THE STORY: Don't believe the premise - the movie has nothing to do with abandoned cars, and people finially understanding what the mysterious happenings are. It's a draub, basic, go to cabin movie with no intensity or \"effort\".  THE SCREENPLAY: I usually give credit to indie screenwriters, it's hard work when you are starting out...but this is crap. The story is flat - it leaves you emotionless the entire movie. The dialogue is extremely weak and predictable boasting lines of \"Woah, you totally freaked me out\" and \"I was wondering if you'd uh...if you'd like to..uh, would you come to the cabin with me?\". It makes me want to rip out all my hair, one strand at a time and feed it to myself.  THE CHARACTERS: HOLY CRAP!!!! Some have described the characters as flat, I want to take it one step further and say that they actually have a reverse character arch.. They actually start working on a parallel universe and almost start acting backwards...  THE ACTORS: Worse than the characters are the actors. They take already poor written characters and add in terrible high school drama acting. The \"Woah you totally freaked me out\" was said so monotone and slow - like it was dumbed down. I could complain for hours on the actors alone.  TECHNICAL: LIGHTING: An eight year old would be disappointed with lighting on this movie. Too shadowy in areas, too bleached in others. The director shouldn't use light as an emotion until he learns how to light a basic scene properly. Baby Steps! SOUND: How many sound guys does it take to make a really shotty sounding movie? 9. With that many sound guys this should sound amazing but quite the opposite has occured. There is one scene in particular that really sticks out, these guys are driving in a car and the sound of the car changes with every camera angle....WEAK! CAMERA: Learn to use it.  Anyway, I'm running out of complaining space.....rent it - I dare you...Rent it and learn from it...give it a 1 rating..it deserves it.  Signing off... Amanda Christmas\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO6D9NuMi4II"
      },
      "source": [
        "Применим ее:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LTwQqs_hD-K"
      },
      "source": [
        "train_df['review'] = train_df['review'].apply(lambda text: pattern.subn(' ', text)[0])\n",
        "test_df['review'] = test_df['review'].apply(lambda text: pattern.subn(' ', text)[0])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6WVgK4LtUn2"
      },
      "source": [
        "def eval_model(model, test):\n",
        "      preds = model.predict(test['review'])\n",
        "      return accuracy_score(test['is_positive'], preds)\n",
        "      \n",
        "def count_vectorizer_classifier(X, y, test):\n",
        "  vectorizer = CountVectorizer()\n",
        "  classifier = LogisticRegression()\n",
        "\n",
        "  model = Pipeline([\n",
        "      ('vectorizer', vectorizer),\n",
        "      ('classifier', classifier)\n",
        "  ])\n",
        "\n",
        "  model.fit(train_df['review'], train_df['is_positive'])\n",
        "\n",
        "  return eval_model(model, test)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbFXKNrngP46"
      },
      "source": [
        "## Придумываем новые признаки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz7GSFzIlv4V"
      },
      "source": [
        "### Tf-idf\n",
        "\n",
        "Сейчас мы на все слова смотрим с одинаковым весом - хотя какие-то из них более редкие, какие-то более частые, и эта частотность - полезная, вообще говоря, информация.\n",
        "\n",
        "Самый простой способ добавить статистическую информацию о частотностях - сделать *tf-idf* взвешивание:\n",
        "\n",
        "$$\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t)$$\n",
        "\n",
        "*tf* - term-frequency - частотность слова `t` в конкретном документе `d` (рецензии в нашем случае). Это ровно то, что мы уже считали.\n",
        "\n",
        "*idf* - inverse document-frequency - коэффициент, который тем больше, чем в меньшем числе документов встречалось данное слово. Считается как-нибудь так:\n",
        "$$\\text{idf}(t) = \\text{log}\\frac{1 + n_d}{1 + n_{d(t)}} + 1$$\n",
        "где $n_d$ - число всех документов, а $n_{d(t)}$ - число документов со словом `t`.\n",
        "\n",
        "Использовать его просто - нужно заменить `CountVectorizer` на `TfidfVectorizer`.\n",
        "\n",
        "**Задание** Попробуйте запустить `TfidfVectorizer`. Посмотрите на ошибки, которые он научился исправлять, и на ошибки, которые он начал делать - по сравнению с `CountVectorizer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3DjjiJglvT3"
      },
      "source": [
        "def tfidf_classifier(X, y, test):\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  classifier = LogisticRegression()\n",
        "\n",
        "  model = Pipeline([\n",
        "      ('vectorizer', vectorizer),\n",
        "      ('classifier', classifier)\n",
        "  ])\n",
        "\n",
        "  model.fit(X, y)\n",
        "\n",
        "  return eval_model(model, test)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe_CJxQ0tFP9"
      },
      "source": [
        "### N-граммы слов\n",
        "\n",
        "До сих пор мы смотрели на тексты как на мешок слов - но очевидно, что есть разница между `good movie` и `not good movie`.\n",
        "\n",
        "Добавим информацию (хоть какую-то) о последовательностях слов - будем извлекать еще и биграммы слов.\n",
        "\n",
        "В Vectorizer'ах для этого есть параметр `ngram_range=(n_1, n_2)` - он говорит, что нужны n_1-...n_2-граммы.\n",
        "\n",
        "**Задание** Попробуйте увеличенный range и поинтерпретируйте полученный результат."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDpdrT0HuKYN"
      },
      "source": [
        "def words_n_gramm_classifier(X, y, test):\n",
        "  vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "  classifier = LogisticRegression()\n",
        "\n",
        "  model = Pipeline([\n",
        "      ('vectorizer', vectorizer),\n",
        "      ('classifier', classifier)\n",
        "  ])\n",
        "\n",
        "  model.fit(X, y)\n",
        "\n",
        "  return eval_model(model, test)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrBoThj6wl2F"
      },
      "source": [
        "### N-граммы символов\n",
        "\n",
        "Символьные n-граммы дают простой способ выучить полезные корни и суффиксы, не связываясь с этой вашей лингвистикой - только статистика, только хардкор.\n",
        "\n",
        "Например, слово `badass` мы можем представить в виде такой последовательности триграмм:\n",
        "\n",
        "`##b #ba bad ada das ass ss# s##`\n",
        "\n",
        "So interpretable, неправда ли?\n",
        "\n",
        "Реализовать это дело всё так же просто - нужно поставить `analyzer='char'` в вашем любимом Vectorizer'е и выбрать размер `ngram_range`.\n",
        "\n",
        "**Задание** Запилите классификатор на n-граммах символов и визуализируйте его."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFaWmUrGyY3n"
      },
      "source": [
        "def char_n_gramm_classifier(X, y, test):\n",
        "  vectorizer = TfidfVectorizer(ngram_range=(2, 6), max_features=20000, analyzer='char')\n",
        "  classifier = LogisticRegression()\n",
        "\n",
        "  model = Pipeline([\n",
        "      ('vectorizer', vectorizer),\n",
        "      ('classifier', classifier)\n",
        "  ])\n",
        "\n",
        "  model.fit(X, y)\n",
        "\n",
        "  return eval_model(model, test)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsXIosZi-VXh",
        "outputId": "fa3da6a8-8448-4dbf-d79f-a092708a5090"
      },
      "source": [
        "count_vec_cls = count_vectorizer_classifier(train_df['review'], train_df['is_positive'], test_df)\n",
        "tfidf_cls = tfidf_classifier(train_df['review'], train_df['is_positive'], test_df)\n",
        "word_n_gramm_cls = words_n_gramm_classifier(train_df['review'], train_df['is_positive'], test_df)\n",
        "char_n_gramm_cls = char_n_gramm_classifier(train_df['review'], train_df['is_positive'], test_df)\n",
        "\n",
        "print(f'Count vectorizer: {count_vec_cls}')\n",
        "print(f'Tfidf vectorizer: {tfidf_cls}')\n",
        "print(f'Words n-gramm vectorizer: {word_n_gramm_cls}')\n",
        "print(f'Char n-gramm vectorizer: {char_n_gramm_cls}')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Count vectorizer: 0.865\n",
            "Tfidf vectorizer: 0.88284\n",
            "Words n-gramm vectorizer: 0.88644\n",
            "Char n-gramm vectorizer: 0.87856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fZ6I8mN0VPU"
      },
      "source": [
        "## Подключаем лингвистику"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkywIvbp4N-L"
      },
      "source": [
        "### Лемматизация и стемминг\n",
        "\n",
        "Если присмотреться, можно найти формы одного слова с разной семантической окраской по мнению классификатора. Или нет?\n",
        "\n",
        "**Задание** Найти формы слова с разной семантической окраской.\n",
        "\n",
        "Поверя, что они есть, попробуем что-нибудь с этим сделать.\n",
        "\n",
        "Например, лемматизируем - сведем к начальной форме все слова. Поможет в этом библиотека spacy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5CTTdtxI5yv"
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load('en', disable=['parser'])\n",
        "\n",
        "docs = [doc for doc in nlp.pipe(train_df.review.values[:50])]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Pfup3O5r30m"
      },
      "source": [
        "**Задание** Сделайте классификатор на лемматизированных текстах.\n",
        "\n",
        "Более простой способ нормализации слов - использовать стемминг. Он немного тупой, не учитывает контекст, но иногда оказывается даже эффективнее лемматизации - а, главное, быстрее.\n",
        "\n",
        "По сути это просто набор правил, как обрезать слово, чтобы получить основу (stem):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr0w_hVyrqFx"
      },
      "source": [
        "def stemmed_classifier(X, y, test):\n",
        "  stemmer = PorterStemmer()\n",
        "  vectorizer = TfidfVectorizer(ngram_range=(2, 6), max_features=20000, analyzer='char')\n",
        "  classifier = LogisticRegression()\n",
        "\n",
        "  model = Pipeline([\n",
        "      ('stemmer', stemmer),\n",
        "      ('vectorizer', vectorizer),\n",
        "      ('classifier', classifier)\n",
        "  ])\n",
        "\n",
        "  model.fit(X, y)\n",
        "\n",
        "  return eval_model(model, test)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puhKtbH8sm6y"
      },
      "source": [
        "stemmed_classifier(train_df['review'], train_df['is_positive'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxGSEqHNsfHy"
      },
      "source": [
        "**Задание** Попробуйте вместо лемм классифицировать основы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1NBpkwuspBX"
      },
      "source": [
        "### NER\n",
        "\n",
        "В текстах рецензий очень много именованных сущностей. Вот, например:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "id": "Xki8Maf9MrVY",
        "outputId": "f0988ff1-b192-444b-d578-f8596e529b45"
      },
      "source": [
        "displacy.render(docs[0], style='ent', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Dreamgirls\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ", despite its fistful of \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tony\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " wins in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    an incredibly weak year\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " on \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Broadway\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
              "</mark>\n",
              ", has never been what one would call a jewel in the crown of stage musicals. However, that is not to say that in the right cinematic hands it could not be fleshed out and polished into something worthwhile on-screen. Unfortunately, what transfers to the screen is basically a slavishly faithful version of the stage hit with all of its inherent weaknesses intact. \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    First\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              ", the score has never been one of the strong points of this production and the film does not change that factor. There are lots of songs (perhaps too many?), but few of them are especially memorable. The closest any come to catchy tunes are the title song and \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    One Night\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
              "</mark>\n",
              " Only - the much acclaimed And I Am Telling You That I Am Not Going is less a great song than it is a dramatic set piece for the character of \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Effie\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " (\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Jennifer Hudson\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              "). The film is slick and technically well-produced, but the story and characters are surprisingly thin and lacking in any resonance. There is some interest in the opening moments, watching \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Jamie Foxx's\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " Svengali-like manager manipulate his acts to the top, but that takes a back seat in the latter portion of the film, when the story conveniently tries to cast him as a villain, despite his having been right from a business stand-point for a good majority of the film. \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Beyonce Knowles\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " is lovely and sings her songs perfectly well, but is stuck with a character who is basically all surface glitz. \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Anika Noni Rose\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " as the \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    third\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              " member of the \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Dreamgirls\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " trio literally has nothing to do for the entire film. \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Eddie Murphy\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " acquits himself well as a singer obviously based on \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    James Brown\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", but the role is not especially meaty and ultimately has little impact. \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Foxx\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " would seem ideal casting, but he seems oddly withdrawn and bored. The film's biggest selling point is surely former \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    American Idol\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " contestant/\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Oscar\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " winner \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Jennifer Hudson\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " in the central role of \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Effie White\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", the temperamental singer who gets booted from the group and makes a triumphant closing act return. For me, \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Effie\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " has always been a big problem in both the show and the movie. The film obviously wants you to feel sorry for her and rather ham-handedly takes her side, but I have never been sure that this character deserves that kind of devotion. From the start, \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Effie\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " conducts herself for the most part like an obnoxious, egotistical, self-centered diva, who is more interested in what everyone else can do for her rather than having much vested interest in the group of which she is a part. When she is booted from the group for her unprofessionalism and bad attitude, the charges are more than well-founded, but the stage show/film seem to think \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Effie\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " should be cut unlimited slack simply because she has a great voice. Even though the film tries to soften some of \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Effie\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              "'s harder edges to make her more likable, the charges still stand. Her story becomes more manipulative by suggesting she should have our further sympathy because she is an unwed mother struggling to raise her daughter - using the implication that (much like the talent card) motherhood immediately makes any behavior excusable. Indeed the only big effort the film makes to show \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Effie\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              "'s mothering is to tell us about it and then include a scene where she barks at her daughter in the unemployment office, insists that the girl has &quot;no father&quot; and then refuse to look for gainful employment to support them since singing is all she knows. In the hands of a skillful actress, the gaps could perhaps have been remedied with technique and charisma. Unfortunately, \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Hudson\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " is not that actress. She sings well, but the dialog-driven moments do not come naturally to her nor do high emotional moments. \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Effie\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              "'s signature moment (the aforementioned And I Am Telling You... number) is well-sung by \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Hudson\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", but emotionally flat in the acting department. \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Effie\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " is supposed to expressing her rage and desperation at her predicament, but \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Hudson\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " comes off as a cabaret performer belting out a hot number. All in all, not quite the emotional highlight one expects. The latter portion of the film is basically a predictable melange of events that maneuver \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Foxx\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " into \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Hudson\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "'s earlier position and allow her to strut back in and lord it over everyone. \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Foxx\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "'s criminal offenses in the film are undoubtedly par for the course of many struggling record producers, but the film's seeming implication that he has it coming because he helped usher in the disco era is rather ridiculous, not to mention pretentious and condescending, particularly coming from a film with all of the depth of a puddle. The end result is a faithful rendition of the stage hit, drained of emotion, energy or anything that can be described as dynamic.</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGhcnuZSteHc"
      },
      "source": [
        "Вообще говоря, почему вдруг какой-нибудь Депп должен нести семантическую окраску? Однако оказывается, что классификатор выучивает, что какие-то имена чаще в положительных рецензиях - или наоборот. Это похоже на переобучение - почему бы не попробовать вырезать сущности?\n",
        "\n",
        "**Задание** Удалите из текстов какие-то из сущностей, пользуясь координатами из запикленных файлов. Описание сущностей можно посмотреть [здесь](https://spacy.io/api/annotation#named-entities). Запустите классификатор."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2fHA70b0zEZ"
      },
      "source": [
        "## Включаем дип лёрнинг"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1VCrM671XO5"
      },
      "source": [
        "Мы тут пришли deep learning'ом заниматься, а делаем почему-то модель на логистической регрессии. Как так?\n",
        "\n",
        "Попробуем запустить относительно стандартную модель для классификации текстов - сверточная сеть поверх словных эмбеддингов.\n",
        "\n",
        "Разбираться, что это за зверь, будем на следующих занятиях, а пока будем просто им пользоваться :)\n",
        "\n",
        "Каждое предложение нужно представлять набором слов - и сразу же начинаются проблемы. Во-первых, как ограничить длину предложения?\n",
        "\n",
        "Прикинем по гистограмме, какая длина нам подходит:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02Kv8YUPbhzG",
        "outputId": "457dbb1c-caff-4dcc-8735-3cc78098efcf"
      },
      "source": [
        "train_df['review']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        Dreamgirls, despite its fistful of Tony wins i...\n",
              "1        This show comes up with interesting locations ...\n",
              "2        I simply love this movie. I also love the Ramo...\n",
              "3        Spoilers ahead if you want to call them that.....\n",
              "4        My all-time favorite movie! I have seen many m...\n",
              "                               ...                        \n",
              "24995    I am a big fan of the movie, but not for the u...\n",
              "24996    I'm not going to bother with a plot synopsis s...\n",
              "24997    This movie . . . I don't know. Why they would ...\n",
              "24998    Saw this film on DVD yesterday and was gob-sma...\n",
              "24999    This was a disappointment - none of the nuance...\n",
              "Name: review, Length: 25000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "O477ZV1t1WIO",
        "outputId": "fb1053f0-abc6-4b91-cd6d-9b6849da2e3b"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "_, _, hist = plt.hist(train_df.review.apply(lambda text: len(text.split())), bins='auto')\n",
        "hist"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<a list of 231 Patch objects>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXO4xi0u5m8l"
      },
      "source": [
        "Кроме этого, нужно перенумеровать как-то слова."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIMGE7L-55fs",
        "outputId": "c399154c-82ae-4ff2-9c69-1a9a7c5a81c4"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "words_counter = Counter((word for text in train_df.review for word in text.lower().split()))\n",
        "\n",
        "word2idx = {\n",
        "    '': 0,\n",
        "    '<unk>': 1\n",
        "}\n",
        "for word, count in words_counter.most_common():\n",
        "    if count < 10:\n",
        "        break\n",
        "        \n",
        "    word2idx[word] = len(word2idx)\n",
        "    \n",
        "print('Words count', len(word2idx))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words count 26783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTptlmd1yD3J"
      },
      "source": [
        "**Задание** Сконвертируйте данные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPP0cYdJ5VkE"
      },
      "source": [
        "def convert(texts, word2idx, max_text_len):\n",
        "    data = np.zeros((len(texts), max_text_len), dtype=np.int)\n",
        "    \n",
        "    for inx, text in enumerate(texts):\n",
        "        result = []\n",
        "        for word in text.split():\n",
        "            if word in word2idx:\n",
        "                result.append(word2idx[word])\n",
        "        padding = [0]*(max_text_len - len(result))\n",
        "        data[inx] = np.array(padding + result[-max_text_len:], dtype=np.int)\n",
        "    return data\n",
        "\n",
        "X_train = convert(train_df.review, word2idx, 1000)\n",
        "X_test = convert(test_df.review, word2idx, 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYb-p5ioyLUZ"
      },
      "source": [
        "Поставим учиться модельку на keras.\n",
        "\n",
        "*Напоминание*: на keras, чтобы обучить модель, нужно\n",
        "1. Определить модель, например:\n",
        "```python \n",
        "model = Sequential()\n",
        "model.add(Dense(1, activation='sigmoid', input_dim=NUM_WORDS))\n",
        "```\n",
        "2. Задать функцию потерь и оптимизатор:\n",
        "```python\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "3. Запустить обучение:\n",
        "```python\n",
        "model.fit(X_train, y_train, \n",
        "          batch_size=32,\n",
        "          epochs=3,\n",
        "          validation_data=(X_test, y_test))\n",
        "```\n",
        "\n",
        "В NLP чаще всего ставятся задачи классификации, поэтому нужно запомнить такие функции потерь:\n",
        "\n",
        "*   **categorical_crossentropy** - для многоклассовой классификации, в качестве меток должны передаваться one-hot-encoding вектора\n",
        "*   **sparse_categorical_crossentropy** - аналогично предыдущему, но в качестве меток нужно передавать просто индексы соответствующих классов\n",
        "*   **binary_crossentropy** - для бинарной классификации\n",
        "\n",
        "\n",
        "В качестве оптимизатора обычно используют `sgd` или `adam`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ncrq09zRxEvN",
        "outputId": "453b501d-c0bf-414c-9e9a-667eddcc38e8"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KddjCCo-w50h"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalMaxPooling1D, Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDjri3167vFf",
        "outputId": "a82ade6e-49f1-496f-e80a-ed1b590cfc69"
      },
      "source": [
        "model = Sequential([\n",
        "    Embedding(input_dim=len(word2idx), output_dim=64, input_shape=(X_train.shape[1],)),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(units=10, activation='relu'),\n",
        "    Dense(units=10, activation='relu'),\n",
        "    \n",
        "    Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 1000, 64)          1714112   \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_3 (Glob (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                650       \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 1,714,883\n",
            "Trainable params: 1,714,883\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lw93gTmq8gZl",
        "outputId": "e10a2d0b-0287-4295-e796-41943209d305"
      },
      "source": [
        "model.fit(X_train, train_df.is_positive, batch_size=128, epochs=10, \n",
        "          validation_data=(X_test, test_df.is_positive))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "196/196 [==============================] - 12s 58ms/step - loss: 0.6695 - accuracy: 0.6759 - val_loss: 0.4019 - val_accuracy: 0.8374\n",
            "Epoch 2/10\n",
            "196/196 [==============================] - 12s 61ms/step - loss: 0.3275 - accuracy: 0.8725 - val_loss: 0.2989 - val_accuracy: 0.8732\n",
            "Epoch 3/10\n",
            "196/196 [==============================] - 12s 63ms/step - loss: 0.1903 - accuracy: 0.9302 - val_loss: 0.2959 - val_accuracy: 0.8743\n",
            "Epoch 4/10\n",
            "196/196 [==============================] - 13s 65ms/step - loss: 0.1074 - accuracy: 0.9668 - val_loss: 0.3193 - val_accuracy: 0.8736\n",
            "Epoch 5/10\n",
            "196/196 [==============================] - 13s 67ms/step - loss: 0.0514 - accuracy: 0.9896 - val_loss: 0.3587 - val_accuracy: 0.8708\n",
            "Epoch 6/10\n",
            "196/196 [==============================] - 13s 66ms/step - loss: 0.0216 - accuracy: 0.9976 - val_loss: 0.3995 - val_accuracy: 0.8689\n",
            "Epoch 7/10\n",
            "196/196 [==============================] - 13s 67ms/step - loss: 0.0090 - accuracy: 0.9995 - val_loss: 0.4358 - val_accuracy: 0.8676\n",
            "Epoch 8/10\n",
            "196/196 [==============================] - 13s 66ms/step - loss: 0.0044 - accuracy: 0.9999 - val_loss: 0.4668 - val_accuracy: 0.8670\n",
            "Epoch 9/10\n",
            "196/196 [==============================] - 13s 66ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.4886 - val_accuracy: 0.8673\n",
            "Epoch 10/10\n",
            "196/196 [==============================] - 13s 64ms/step - loss: 0.0019 - accuracy: 0.9999 - val_loss: 0.5098 - val_accuracy: 0.8670\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8a3a636150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBGdVRQTyynD"
      },
      "source": [
        "**Задание** Подсчитайте качество модели на тесте"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnPrvmkh0tmM"
      },
      "source": [
        "model.eval_metrics(X_test, test_df.is_positive)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-Yhlat_1HrZ"
      },
      "source": [
        "1. Посмотрите на токены если будут мусорные добавьте их в стоп слова и обучите заново\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "2. Проверьте повысилось ли качество на стандартных подходах при лемматизации/и без неё\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "3. Удалите/(замените на тег) из текстов сущности(имена, локации, что-то ещё). Запустите классификатор и модельки на сеточках\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "4. Сделайте выводы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyUJj-A_1GWi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}